{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Проблемы на данный момент\n",
    "1) Необходимо что-то сделать с токенизатором или с изначальным разделением на токены так как есть проблемы с его обратным конвертированием\n",
    "2) Модель чаще всего предсказывает куски, необходим алгоритм поиска или дообучение\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import random\n",
    "from spacy.training import offsets_to_biluo_tags, biluo_to_iob\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.lang.ru import Russian\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import pipeline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': 809436509,\n 'text': 'Извещение о проведении открытого конкурса в электронной форме для закупки №0328300032822000806 Общая информация Номер извещения 0328300032822000806 Наименование объекта закупки Поставка продуктов питания Способ определения поставщика (подрядчика, исполнителя) Открытый конкурс в бль Порядок внесения денежных средств в качестве обеспечения заявки на участие в закупке, а также условия гарантии Обеспечение заявки на участие в закупке может предоставляться участником закупки в виде денежных средств или независимой гарантии, предусмотренной ст. 45 Федерального закона № 44-ФЗ. Выбор способа обеспечения осуществляется участником закупки самостоятельно. Срок действия независимой гарантии должен составлять не менее месяца с даты окончания срока подачи заявок. Обеспечение заявки на участие в закупке предоставляется в соответствии с ч. 5 ст. 44 Федерального закона № 44-ФЗ. Условия независимой гарантии в соответствии со ст. 45 Федерального закона № 44-ФЗ. Реквизиты счета в соответствии с п.16 ч. 1 ст. 42 Закона № 44-ФЗ \"Номер расчётного счёта\" 00000000000000000000 \"Номер лицевого счёта\" См. прилагаемые документы \"БИК\" 000000000 \"Наименование кредитной организации\" \"Номер корреспондентского счета\" Обеспечение исполнения контракта Требуется обеспечение исполнения контракта Размер обеспечения исполнения контракта 6593.25 Российский рубль Порядок обеспечения исполнения контракта, требования к обеспечению Исполнение контракта, гарантийные обязательства могут обеспечиваться предоставлением независимой гарантии, соответствующей требованиям ст. 45 Федерального закона № 44-ФЗ, или внесением денежных средств на указанный заказчиком счет, на котором в соответствии с законодательством Российской Федерации учитываются операции со средствами, поступающими заказчику. Способ обеспечения исполнения контракта, гарантийных обязательств, срок действия независимой гарантии определяются в соответствии с требованиями Федерального закона № 44-ФЗ участником закупки, с которым заключается контракт, самостоятельно. При этом срок действия независимой гарантии должен превышать предусмотренный контрактом срок исполнения обязательств, которые должны быть обеспечены такой независимой гарантией, не менее чем на один месяц, в том числе в случае его изменения в соответствии со ст. 95 Федерального закона № 44-ФЗ. Порядок предоставления и требования о Информация и документы, подтверждающие соответствие участников закупки дополнительным требованиям: 1) исполненный договор; 2) акт приемки оказанных услуг и (или) поставленных товаров, подтверждающий цену оказанных услуг и (или) поставленных товаров 4',\n 'label': 'обеспечение исполнения контракта',\n 'extracted_part': {'text': ['Размер обеспечения исполнения контракта 6593.25 Российский рубль'],\n  'answer_start': [1279],\n  'answer_end': [1343]}}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dataset\\\\train.json', 'r', encoding='utf-8') as f: #открыли файл с данными\n",
    "    data = json.load(f) #загнали все, что получилось в переменную\n",
    "data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"РАЗДЕЛ 4. тельно); в) 1 процент цены контракта, ес...\" with entities \"[(1010, 1179, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"PAGE ПРОЕКТ КОНТРАКТА КОНТРАКТ № ______ на поставк...\" with entities \"[(1226, 1355, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"ПРОЕКТ Государственный контракт №________ на поста...\" with entities \"[(1193, 1314, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"УТВЕРЖДАЮ: Директор МКУ «ХЭУ АСР» __________ А.В. ...\" with entities \"[(1277, 1356, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Приложение № 4 к извещению Проект МУНИЦИПАЛЬНЫЙ КО...\" with entities \"[(1290, 1409, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"СОГЛАСОВАНО УТВЕРЖДЕНО И.о.начальника МКУ Директор...\" with entities \"[(1196, 1308, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"1 31 Приложение № 2 ПРОЕКТ КОНТРАКТА на организаци...\" with entities \"[(1243, 1355, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"ПРОЕКТ Контракт № _______________ Поставка хозяйст...\" with entities \"[(1261, 1384, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Утверждаю: Заведующий структурным подразделением Г...\" with entities \"[(1247, 1330, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"УТВЕРЖДАЮ Врио генерального директора ООО «ТСАХ» _...\" with entities \"[(1255, 1323, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"ЧАСТЬ II: Проект договора Запрос ценовых котировок...\" with entities \"[(1233, 1322, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"8 ПРОЕКТ Договор поставки товаров № ________ г. Ха...\" with entities \"[(1231, 1313, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"7 Договор поставки №______ г. Краснотурьинск «____...\" with entities \"[(1069, 1181, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"УТВЕРЖДАЮ Директор ГБУ «Жилищник Обручевского райо...\" with entities \"[(1070, 1170, 'EC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Проект муниципального контракта №_____ на выполнен...\" with entities \"[(1263, 1485, 'PWO')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"УТВЕРЖДАЮ Заказчик Главный врач ГБУЗ ООД Р.А. Зубк...\" with entities \"[(1147, 1318, 'PWO')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"9 Проект КОНТРАКТ № _______ Новосибирская область ...\" with entities \"[(1247, 1440, 'PWO')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Приложение №3 к извещению о проведении электронног...\" with entities \"[(1187, 1349, 'PWO')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Приложение к распоряжению администрации МО «Хараба...\" with entities \"[(1241, 1390, 'PWO')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Приложение №4 к извещению об осуществлении закупки...\" with entities \"[(1271, 1379, 'PWO')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"11 21 УТВЕРЖДАЮ Директор СПб ГБУ СОН «КЦСОН Фрунзе...\" with entities \"[(1215, 1376, 'PWO')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"14 МИНИСТЕРСТВО ЗДРАВООХРАНЕНИЯ РОССИЙСКОЙ ФЕДЕРАЦ...\" with entities \"[(1258, 1560, 'PWO')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Контракт ________________ «___» _________ ____ г. ...\" with entities \"[(1277, 1368, 'PWO')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Версия с 04.07.2022 года У Т В Е Р Ж Д А Ю «ГАУ РС...\" with entities \"[(1183, 1379, 'PWO')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Информационная карта о проведении закупки с единст...\" with entities \"[(1246, 1413, 'PWO')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"«УТВЕРЖДАЮ» ______________/___________/ «__» _____...\" with entities \"[(1253, 1412, 'PWO')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "labels_ner = {'обеспечение исполнения контракта': 'EC', 'обеспечение гарантийных обязательств': 'PWO'}\n",
    "\n",
    "def create_dataset(data):\n",
    "    nlp = spacy.blank(\"ru\")\n",
    "    max_lengh_tag = 0\n",
    "    count = 0\n",
    "    ner_data = []\n",
    "    unique_list = []\n",
    "\n",
    "    for dict_data in data:\n",
    "        #d = {'sentence_id': None, 'words': None, 'labels':None}\n",
    "        doc = nlp(dict_data.get('text'))\n",
    "        #for token in doc:\n",
    "        #print([t.text for t in doc])\n",
    "        #print('Длинна токена: ',len(doc))\n",
    "\n",
    "        if len(doc) > 512:\n",
    "            count+=1\n",
    "        if max_lengh_tag < len(doc):\n",
    "            max_lengh_tag = len(doc)\n",
    "\n",
    "        label = labels_ner.get(dict_data.get('label'))\n",
    "        answer_start = dict_data.get('extracted_part').get('answer_start')[0]\n",
    "        answer_end = dict_data.get('extracted_part').get('answer_end')[0]\n",
    "\n",
    "        entities = [(answer_start, answer_end, label)]\n",
    "        tags = offsets_to_biluo_tags(doc, entities)\n",
    "        iob_tags = list(biluo_to_iob(tags)) # Переводим в формат IOB\n",
    "\n",
    "        if answer_start!=0 and answer_end!=0 and '-' not in iob_tags:\n",
    "            words = [t.text for t in doc]\n",
    "            ner_data.append({'tokens': words, 'tags': iob_tags})\n",
    "\n",
    "            unique_list += iob_tags\n",
    "            unique_list = list(set(unique_list))\n",
    "\n",
    "    return ner_data, unique_list\n",
    "\n",
    "''' Создаём список из всех уникальных тегов и необходимый формат данных'''\n",
    "ner_data, tags_list = create_dataset(data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-PWO', 'O', 'I-EC', 'B-EC', 'I-PWO']\n"
     ]
    }
   ],
   "source": [
    "print(tags_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Извещение', 'о', 'проведении', 'открытого', 'конкурса', 'в', 'электронной', 'форме', 'для', 'закупки', '№', '0328300032822000806', 'Общая', 'информация', 'Номер', 'извещения', '0328300032822000806', 'Наименование', 'объекта', 'закупки', 'Поставка', 'продуктов', 'питания', 'Способ', 'определения', 'поставщика', '(', 'подрядчика', ',', 'исполнителя', ')', 'Открытый', 'конкурс', 'в', 'бль', 'Порядок', 'внесения', 'денежных', 'средств', 'в', 'качестве', 'обеспечения', 'заявки', 'на', 'участие', 'в', 'закупке', ',', 'а', 'также', 'условия', 'гарантии', 'Обеспечение', 'заявки', 'на', 'участие', 'в', 'закупке', 'может', 'предоставляться', 'участником', 'закупки', 'в', 'виде', 'денежных', 'средств', 'или', 'независимой', 'гарантии', ',', 'предусмотренной', 'ст.', '45', 'Федерального', 'закона', '№', '44-ФЗ', '.', 'Выбор', 'способа', 'обеспечения', 'осуществляется', 'участником', 'закупки', 'самостоятельно', '.', 'Срок', 'действия', 'независимой', 'гарантии', 'должен', 'составлять', 'не', 'менее', 'месяца', 'с', 'даты', 'окончания', 'срока', 'подачи', 'заявок', '.', 'Обеспечение', 'заявки', 'на', 'участие', 'в', 'закупке', 'предоставляется', 'в', 'соответствии', 'с', 'ч', '.', '5', 'ст.', '44', 'Федерального', 'закона', '№', '44-ФЗ', '.', 'Условия', 'независимой', 'гарантии', 'в', 'соответствии', 'со', 'ст.', '45', 'Федерального', 'закона', '№', '44-ФЗ', '.', 'Реквизиты', 'счета', 'в', 'соответствии', 'с', 'п.16', 'ч', '.', '1', 'ст.', '42', 'Закона', '№', '44-ФЗ', '\"', 'Номер', 'расчётного', 'счёта', '\"', '00000000000000000000', '\"', 'Номер', 'лицевого', 'счёта', '\"', 'См', '.', 'прилагаемые', 'документы', '\"', 'БИК', '\"', '000000000', '\"', 'Наименование', 'кредитной', 'организации', '\"', '\"', 'Номер', 'корреспондентского', 'счета', '\"', 'Обеспечение', 'исполнения', 'контракта', 'Требуется', 'обеспечение', 'исполнения', 'контракта', 'Размер', 'обеспечения', 'исполнения', 'контракта', '6593.25', 'Российский', 'рубль', 'Порядок', 'обеспечения', 'исполнения', 'контракта', ',', 'требования', 'к', 'обеспечению', 'Исполнение', 'контракта', ',', 'гарантийные', 'обязательства', 'могут', 'обеспечиваться', 'предоставлением', 'независимой', 'гарантии', ',', 'соответствующей', 'требованиям', 'ст.', '45', 'Федерального', 'закона', '№', '44-ФЗ', ',', 'или', 'внесением', 'денежных', 'средств', 'на', 'указанный', 'заказчиком', 'счет', ',', 'на', 'котором', 'в', 'соответствии', 'с', 'законодательством', 'Российской', 'Федерации', 'учитываются', 'операции', 'со', 'средствами', ',', 'поступающими', 'заказчику', '.', 'Способ', 'обеспечения', 'исполнения', 'контракта', ',', 'гарантийных', 'обязательств', ',', 'срок', 'действия', 'независимой', 'гарантии', 'определяются', 'в', 'соответствии', 'с', 'требованиями', 'Федерального', 'закона', '№', '44-ФЗ', 'участником', 'закупки', ',', 'с', 'которым', 'заключается', 'контракт', ',', 'самостоятельно', '.', 'При', 'этом', 'срок', 'действия', 'независимой', 'гарантии', 'должен', 'превышать', 'предусмотренный', 'контрактом', 'срок', 'исполнения', 'обязательств', ',', 'которые', 'должны', 'быть', 'обеспечены', 'такой', 'независимой', 'гарантией', ',', 'не', 'менее', 'чем', 'на', 'один', 'месяц', ',', 'в', 'том', 'числе', 'в', 'случае', 'его', 'изменения', 'в', 'соответствии', 'со', 'ст.', '95', 'Федерального', 'закона', '№', '44-ФЗ', '.', 'Порядок', 'предоставления', 'и', 'требования', 'о', 'Информация', 'и', 'документы', ',', 'подтверждающие', 'соответствие', 'участников', 'закупки', 'дополнительным', 'требованиям', ':', '1', ')', 'исполненный', 'договор', ';', '2', ')', 'акт', 'приемки', 'оказанных', 'услуг', 'и', '(', 'или', ')', 'поставленных', 'товаров', ',', 'подтверждающий', 'цену', 'оказанных', 'услуг', 'и', '(', 'или', ')', 'поставленных', 'товаров', '4'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EC', 'I-EC', 'I-EC', 'I-EC', 'I-EC', 'I-EC', 'I-EC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "print(ner_data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1172 entries, 0 to 1171\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tokens  1172 non-null   object\n",
      " 1   tags    1172 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 18.4+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294 entries, 0 to 293\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tokens  294 non-null    object\n",
      " 1   tags    294 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 4.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                 tokens  \\\n0     [Приложение, №, 2, к, Извещению, ГОСУДАРСТВЕНН...   \n1     [1, 2, Утверждено, на, заседании, Постоянно, д...   \n2     [Извещение, о, проведении, электронного, аукци...   \n3     [Извещение, о, проведении, электронного, аукци...   \n4     [УТВЕРЖДАЮ, Директор, бюджетного, стационарног...   \n...                                                 ...   \n1167  [УТВЕРЖДАЮ, :, Главный, врач, ГАУЗ, ТО, «, ОЛР...   \n1168  [Проект, договора, №, _, _, _, _, _, _, _, _, ...   \n1169  [1, Приложение, №, 4, к, Извещению, об, электр...   \n1170  [Извещение, о, проведении, электронного, аукци...   \n1171  [ПРОЕКТ, КОНТРАКТ, №, _, _, _, на, поставку, п...   \n\n                                                   tags  \n0     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n3     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n4     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n...                                                 ...  \n1167  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n1168  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n1169  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n1170  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n1171  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n\n[1172 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tokens</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[Приложение, №, 2, к, Извещению, ГОСУДАРСТВЕНН...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[1, 2, Утверждено, на, заседании, Постоянно, д...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Извещение, о, проведении, электронного, аукци...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[Извещение, о, проведении, электронного, аукци...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[УТВЕРЖДАЮ, Директор, бюджетного, стационарног...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1167</th>\n      <td>[УТВЕРЖДАЮ, :, Главный, врач, ГАУЗ, ТО, «, ОЛР...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>1168</th>\n      <td>[Проект, договора, №, _, _, _, _, _, _, _, _, ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>1169</th>\n      <td>[1, Приложение, №, 4, к, Извещению, об, электр...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>1170</th>\n      <td>[Извещение, о, проведении, электронного, аукци...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>1171</th>\n      <td>[ПРОЕКТ, КОНТРАКТ, №, _, _, _, на, поставку, п...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1172 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.2, random_state=12345)\n",
    "\n",
    "ner_train = pd.DataFrame(ner_train)\n",
    "ner_test = pd.DataFrame(ner_test)\n",
    "\n",
    "print(ner_train.info())\n",
    "print(ner_test.info())\n",
    "display(ner_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'tags'],\n        num_rows: 1172\n    })\n    test: Dataset({\n        features: ['tokens', 'tags'],\n        num_rows: 294\n    })\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data = DatasetDict({\n",
    "    'train': Dataset.from_pandas(ner_train),\n",
    "    'test': Dataset.from_pandas(ner_test)\n",
    "})\n",
    "ner_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model_checkpoint = \"cointegrated/rubert-tiny\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1 color=red>Необходимо разобраться</h1>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Специальные токены имеют идентификатор слова None. Мы устанавливаем метку на -100, чтобы они автоматически игнорировались в функции потерь.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # Мы устанавливаем метку для первого токена каждого слова.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # Для других токенов в слове мы устанавливаем метку либо на текущую метку, либо на -100, в зависимости от флага label_all_tokens.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_ids = [tags_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [[2, 21, 22, 299, 11513, 18674, 14380, 548, 650, 2448, 3526, 1768, 1499, 13151, 5290, 948, 28580, 7084, 10208, 650, 24624, 22373, 11543, 16892, 332, 4307, 3683, 295, 20887, 105, 296, 4829, 22797, 613, 17193, 11281, 117, 17, 310, 25364, 11424, 27928, 422, 997, 733, 1004, 18, 1201, 18, 22285, 315, 18, 328, 12666, 3575, 1635, 1154, 1186, 13167, 7084, 10837, 330, 3091, 15577, 7113, 656, 778, 28048, 326, 650, 24624, 1550, 13, 16006, 293, 6325, 15014, 16, 16296, 22069, 1229, 1647, 13361, 25409, 314, 778, 3003, 21901, 326, 2225, 24967, 613, 650, 27353, 603, 3789, 679, 16, 548, 671, 320, 2445, 2225, 18583, 811, 18, 3086, 328, 4992, 17009, 626, 2225, 18583, 603, 329, 958, 6788, 871, 20670, 1512, 18836, 603, 20939, 8558, 5584, 10647, 3118, 3789, 1385, 12, 1395, 656, 6086, 16654, 13, 7857, 5968, 2225, 12666, 17557, 745, 13120, 2095, 10312, 705, 26941, 988, 24921, 12847, 16767, 656, 329, 331, 17009, 761, 4295, 10712, 811, 1781, 650, 1231, 21820, 775, 18, 29, 18, 22, 18, 282, 8896, 7081, 1129, 2225, 24967, 613, 650, 24624, 954, 27133, 8001, 12, 7225, 25039, 13302, 761, 16, 29455, 1635, 1154, 1186, 13167, 7084, 10837, 330, 3091, 15577, 7113, 656, 778, 28048, 326, 650, 24624, 1550, 13, 21992, 27757, 938, 13302, 16, 7305, 1580, 16527, 4136, 20939, 8558, 5584, 10647, 9550, 3789, 13248, 626, 24889, 16, 8611, 13302, 761, 7305, 10763, 4951, 13834, 21010, 866, 5677, 6234, 12029, 26140, 25269, 314, 19183, 20178, 626, 30, 422, 292, 25487, 7244, 12340, 1160, 28691, 296, 8264, 10885, 19864, 12029, 26140, 25269, 314, 8896, 1464, 15169, 613, 13302, 761, 650, 24624, 954, 20939, 8558, 5584, 10647, 5144, 3789, 13248, 1172, 7305, 16896, 298, 3091, 15577, 8672, 705, 21010, 866, 5677, 25149, 12029, 26140, 25269, 280, 5914, 2754, 14273, 5253, 4608, 5677, 6234, 548, 815, 28470, 7154, 312, 5914, 9097, 2095, 327, 9823, 18487, 21, 769, 7305, 5729, 28357, 8626, 721, 769, 7305, 5729, 28357, 8626, 721, 769, 7305, 5729, 28357, 8626, 721, 25, 9, 12, 13388, 13, 733, 293, 6325, 15014, 22, 7305, 5729, 28357, 8626, 721, 769, 7305, 5729, 28357, 8626, 721, 769, 7305, 5729, 28357, 8626, 721, 331, 16717, 4171, 2182, 733, 24466, 20670, 1211, 19864, 12029, 26140, 25269, 314, 21, 16, 25, 12, 7283, 7548, 13, 16495, 16, 1363, 769, 7321, 25, 9, 12, 17236, 13, 733, 293, 6325, 15014, 23, 769, 7305, 5729, 28357, 8626, 721, 7305, 5729, 28357, 8626, 721, 7305, 5729, 28357, 8626, 721, 21010, 866, 5677, 6234, 12029, 26140, 25269, 314, 20178, 626, 312, 5914, 1839, 16, 1363, 769, 7321, 25, 9, 12, 17236, 13, 733, 293, 6325, 15014, 24, 769, 7305, 5729, 28357, 8626, 721, 7305, 5729, 28357, 8626, 721, 769, 7305, 5729, 28357, 8626, 721, 21010, 866, 5677, 6234, 12029, 26140, 25269, 314, 20178, 626, 312, 5914, 1839, 16, 1363, 769, 7321, 25, 9, 12, 17236, 13, 733, 293, 6325, 15014, 25, 7305, 5729, 28357, 8626, 721, 7305, 5729, 28357, 8626, 721, 769, 7305, 5729, 28357, 8626, 721, 331, 16717, 4171, 2182, 733, 24466, 20670, 1211, 19864, 12029, 26140, 25269, 314, 21, 16, 25, 12, 7283, 7548, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100]]}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(ner_data['train'][1:2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1172 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a7342c1c572460385b12dc8fd9822cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/294 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fa569d8bc5d407d9c0000c1cacc6334"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Создаём датасет и хешируем его '''\n",
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "['B-PWO', 'O', 'I-EC', 'B-EC', 'I-PWO']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Создаём модель\n",
    "Теперь, когда наши данные готовы, мы можем загрузить предварительно обученную модель и настроить ее. Поскольку все наши задачи связаны с классификацией токенов, мы используем класс AutoModelForTokenClassification. Как и в случае с токенизатором, метод from_pretrained загрузит и кэширует модель для нас.\n",
    "Единственное, что нам нужно указать, — это количество тегов (IOB) для нашей задачи"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(tags_list))\n",
    "model.config.id2label = dict(enumerate(tags_list))\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Пояснение к коду ниже\n",
    "Затем нам понадобится средство подборки данных, которое объединит наши обработанные примеры вместе, применяя заполнение, чтобы сделать их все одинакового размера (каждое заполнение будет дополнено до длины самого длинного примера). Для этой задачи в библиотеке Transformers есть \"сопоставитель данных\", который дополняет не только входные данные, но и метки:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Загружаем метрику для прогноза\n",
    "Здесь мы загрузим метрику seqeval (которая обычно используется для оценки результатов в наборе данных CONLL) через библиотеку наборов данных."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "{'EC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n 'overall_precision': 1.0,\n 'overall_recall': 1.0,\n 'overall_f1': 1.0,\n 'overall_accuracy': 1.0}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ner_train.to_dict('records')[1]\n",
    "labels = example['tags']\n",
    "\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Таким образом, нам нужно будет провести небольшую постобработку наших прогнозов:\n",
    "\n",
    "* выбираем прогнозируемый индекс (с максимальным логитом) для каждого токена\n",
    "* преобразуем его в строковую метку\n",
    "* игнорируем везде, где мы устанавливаем метку -100\n",
    "\n",
    "Следующая функция выполняет всю эту постобработку результата Trainer.evaluate (который представляет собой namedtuple, содержащий предсказания и метки) перед применением метрики:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Удалить игнорируемый индекс (специальные токены)\n",
    "    true_predictions = [\n",
    "        [tags_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [tags_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 1.4011664390563965,\n 'eval_precision': 0.0018420529861123652,\n 'eval_recall': 0.11929824561403508,\n 'eval_f1': 0.003628085651276944,\n 'eval_accuracy': 0.5384502455690796,\n 'eval_runtime': 4.4228,\n 'eval_samples_per_second': 66.474,\n 'eval_steps_per_second': 4.296}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# разморозка\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "D:\\Jupiter\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1172\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2220\n",
      "  Number of trainable parameters = 11688077\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2220 : < :, Epoch 0.01/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=2220, training_loss=0.09945100964726629, metrics={'train_runtime': 347.831, 'train_samples_per_second': 101.084, 'train_steps_per_second': 6.382, 'total_flos': 248830567096320.0, 'train_loss': 0.09945100964726629, 'epoch': 30.0})"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 294\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 0.04696035012602806,\n 'eval_precision': 0.3514376996805112,\n 'eval_recall': 0.6432748538011696,\n 'eval_f1': 0.4545454545454546,\n 'eval_accuracy': 0.9835642216527867,\n 'eval_runtime': 2.7084,\n 'eval_samples_per_second': 108.551,\n 'eval_steps_per_second': 7.015,\n 'epoch': 30.0}"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ner_bert_30.bin\\config.json\n",
      "Model weights saved in ner_bert_30.bin\\pytorch_model.bin\n",
      "tokenizer config file saved in ner_bert_30.bin\\tokenizer_config.json\n",
      "Special tokens file saved in ner_bert_30.bin\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "('ner_bert_30.bin\\\\tokenizer_config.json',\n 'ner_bert_30.bin\\\\special_tokens_map.json',\n 'ner_bert_30.bin\\\\vocab.txt',\n 'ner_bert_30.bin\\\\added_tokens.json',\n 'ner_bert_30.bin\\\\tokenizer.json')"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('ner_bert_30.bin')\n",
    "tokenizer.save_pretrained('ner_bert_30.bin')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ner_bert_30.bin\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ner_bert_30.bin\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 312,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-PWO\",\n",
      "    \"1\": \"O\",\n",
      "    \"2\": \"I-EC\",\n",
      "    \"3\": \"B-EC\",\n",
      "    \"4\": \"I-PWO\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"label2id\": {\n",
      "    \"B-EC\": 3,\n",
      "    \"B-PWO\": 0,\n",
      "    \"I-EC\": 2,\n",
      "    \"I-PWO\": 4,\n",
      "    \"O\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29564\n",
      "}\n",
      "\n",
      "loading configuration file ner_bert_30.bin\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ner_bert_30.bin\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 312,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-PWO\",\n",
      "    \"1\": \"O\",\n",
      "    \"2\": \"I-EC\",\n",
      "    \"3\": \"B-EC\",\n",
      "    \"4\": \"I-PWO\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"label2id\": {\n",
      "    \"B-EC\": 3,\n",
      "    \"B-PWO\": 0,\n",
      "    \"I-EC\": 2,\n",
      "    \"I-PWO\": 4,\n",
      "    \"O\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29564\n",
      "}\n",
      "\n",
      "loading weights file ner_bert_30.bin\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at ner_bert_30.bin.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "#pipe = pipeline(model=model, tokenizer=tokenizer, task='ner', aggregation_strategy='average', device=0)\n",
    "pipe = pipeline(model='ner_bert_30.bin', tokenizer='ner_bert_30.bin', task='ner', aggregation_strategy='average', device=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def preprocessing_pred(pred):\n",
    "    pred = max([elem.get('word') for elem in pred], key=len)\n",
    "    #print(pred)\n",
    "    pred = pred.replace('( ', '(')\n",
    "    pred = pred.replace('_ ', '_')\n",
    "    pred = pred.replace(' )', ')')\n",
    "\n",
    "    #print('Нужная строка: ', entit)\n",
    "    splt = pred.split()\n",
    "    pattern = splt[0]+'.{1,}'+splt[-1]\n",
    "    try:\n",
    "        pred = re.search(pattern, data[i]['text'])[0]\n",
    "    except:\n",
    "        print('Предсказанная строка: ', pred)\n",
    "        print('Нужная строка: ', entit)\n",
    "        print(re.search(pattern, data[i]['text']))\n",
    "    return pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанная строка:  Размер\n",
      "Нужная строка:  Размер обеспечения исполнения контракта 10.00%\n",
      "None\n",
      "Предсказанная строка:  Размер обеспечения исполнения контракта составляет 10 (Десять) процентов цены контракта, что составляет _____рублей.\n",
      "Нужная строка:  Размер обеспечения исполнения контракта составляет 10 (Десять) процентов цены контракта, что составляет _____ рублей.\n",
      "None\n",
      "Предсказанная строка:  Обеспечение исполнения Контракта устанавливается в размере 0, 5 % от цены Контракта, что составляет ___________(____________)\n",
      "Нужная строка:  Обеспечение исполнения Контракта устанавливается в размере 0,5 % от цены Контракта, что составляет ___________ (____________\n",
      "None\n",
      "Предсказанная строка:  Обеспечение исполнения контракта установлено в размере 5, 00 % от цены контракта на сумму _________рублей\n",
      "Нужная строка:  Обеспечение исполнения контракта установлено в размере 5,00% от цены контракта на сумму _________ рублей.\n",
      "None\n",
      "Предсказанная строка:  Размер обеспечения исполнения Контракта составляет 5 % (\n",
      "Нужная строка:  Размер обеспечения исполнения Контракта составляет 5% (пять процентов) от цены Контракта, что составляет ___________ ( ) рублей\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "missing ), unterminated subpattern at position 11",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31merror\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 12\u001B[0m, in \u001B[0;36mpreprocessing_pred\u001B[1;34m(pred)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 12\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\re.py:200\u001B[0m, in \u001B[0;36msearch\u001B[1;34m(pattern, string, flags)\u001B[0m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;124;03ma Match object, or None if no match was found.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msearch(string)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\re.py:303\u001B[0m, in \u001B[0;36m_compile\u001B[1;34m(pattern, flags)\u001B[0m\n\u001B[0;32m    302\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfirst argument must be string or compiled pattern\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 303\u001B[0m p \u001B[38;5;241m=\u001B[39m \u001B[43msre_compile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (flags \u001B[38;5;241m&\u001B[39m DEBUG):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\sre_compile.py:788\u001B[0m, in \u001B[0;36mcompile\u001B[1;34m(p, flags)\u001B[0m\n\u001B[0;32m    787\u001B[0m     pattern \u001B[38;5;241m=\u001B[39m p\n\u001B[1;32m--> 788\u001B[0m     p \u001B[38;5;241m=\u001B[39m \u001B[43msre_parse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    789\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\sre_parse.py:955\u001B[0m, in \u001B[0;36mparse\u001B[1;34m(str, flags, state)\u001B[0m\n\u001B[0;32m    954\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 955\u001B[0m     p \u001B[38;5;241m=\u001B[39m \u001B[43m_parse_sub\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m&\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mSRE_FLAG_VERBOSE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    956\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Verbose:\n\u001B[0;32m    957\u001B[0m     \u001B[38;5;66;03m# the VERBOSE flag was switched on inside the pattern.  to be\u001B[39;00m\n\u001B[0;32m    958\u001B[0m     \u001B[38;5;66;03m# on the safe side, we'll parse the whole thing again...\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\sre_parse.py:444\u001B[0m, in \u001B[0;36m_parse_sub\u001B[1;34m(source, state, verbose, nested)\u001B[0m\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 444\u001B[0m     itemsappend(\u001B[43m_parse\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnested\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    445\u001B[0m \u001B[43m                       \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnested\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mitems\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sourcematch(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\sre_parse.py:843\u001B[0m, in \u001B[0;36m_parse\u001B[1;34m(source, state, verbose, nested, first)\u001B[0m\n\u001B[0;32m    842\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m source\u001B[38;5;241m.\u001B[39mmatch(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 843\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m source\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmissing ), unterminated subpattern\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    844\u001B[0m                        source\u001B[38;5;241m.\u001B[39mtell() \u001B[38;5;241m-\u001B[39m start)\n\u001B[0;32m    845\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m group \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31merror\u001B[0m: missing ), unterminated subpattern at position 11",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31merror\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[37], line 14\u001B[0m\n\u001B[0;32m      8\u001B[0m     entit \u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pipe(text):\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m#print(pipe(text))\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m#pred = str(pipe(text)[0].get('word'))\u001B[39;00m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;66;03m#print('Предсказанная строка: ', pred)\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocessing_pred\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m#print(pattern)\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;66;03m#print('После поиска: ', pred, end='\\n\\n')\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     18\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "Cell \u001B[1;32mIn[36], line 16\u001B[0m, in \u001B[0;36mpreprocessing_pred\u001B[1;34m(pred)\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mПредсказанная строка: \u001B[39m\u001B[38;5;124m'\u001B[39m, pred)\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mНужная строка: \u001B[39m\u001B[38;5;124m'\u001B[39m, entit)\n\u001B[1;32m---> 16\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pred\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\re.py:200\u001B[0m, in \u001B[0;36msearch\u001B[1;34m(pattern, string, flags)\u001B[0m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msearch\u001B[39m(pattern, string, flags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    198\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msearch(string)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\re.py:303\u001B[0m, in \u001B[0;36m_compile\u001B[1;34m(pattern, flags)\u001B[0m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sre_compile\u001B[38;5;241m.\u001B[39misstring(pattern):\n\u001B[0;32m    302\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfirst argument must be string or compiled pattern\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 303\u001B[0m p \u001B[38;5;241m=\u001B[39m \u001B[43msre_compile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (flags \u001B[38;5;241m&\u001B[39m DEBUG):\n\u001B[0;32m    305\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(_cache) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m _MAXCACHE:\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;66;03m# Drop the oldest item\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\sre_compile.py:788\u001B[0m, in \u001B[0;36mcompile\u001B[1;34m(p, flags)\u001B[0m\n\u001B[0;32m    786\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m isstring(p):\n\u001B[0;32m    787\u001B[0m     pattern \u001B[38;5;241m=\u001B[39m p\n\u001B[1;32m--> 788\u001B[0m     p \u001B[38;5;241m=\u001B[39m \u001B[43msre_parse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    789\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    790\u001B[0m     pattern \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\sre_parse.py:955\u001B[0m, in \u001B[0;36mparse\u001B[1;34m(str, flags, state)\u001B[0m\n\u001B[0;32m    952\u001B[0m state\u001B[38;5;241m.\u001B[39mstr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m\n\u001B[0;32m    954\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 955\u001B[0m     p \u001B[38;5;241m=\u001B[39m \u001B[43m_parse_sub\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m&\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mSRE_FLAG_VERBOSE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    956\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Verbose:\n\u001B[0;32m    957\u001B[0m     \u001B[38;5;66;03m# the VERBOSE flag was switched on inside the pattern.  to be\u001B[39;00m\n\u001B[0;32m    958\u001B[0m     \u001B[38;5;66;03m# on the safe side, we'll parse the whole thing again...\u001B[39;00m\n\u001B[0;32m    959\u001B[0m     state \u001B[38;5;241m=\u001B[39m State()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\sre_parse.py:444\u001B[0m, in \u001B[0;36m_parse_sub\u001B[1;34m(source, state, verbose, nested)\u001B[0m\n\u001B[0;32m    442\u001B[0m start \u001B[38;5;241m=\u001B[39m source\u001B[38;5;241m.\u001B[39mtell()\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 444\u001B[0m     itemsappend(\u001B[43m_parse\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnested\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    445\u001B[0m \u001B[43m                       \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnested\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mitems\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sourcematch(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    447\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\sre_parse.py:843\u001B[0m, in \u001B[0;36m_parse\u001B[1;34m(source, state, verbose, nested, first)\u001B[0m\n\u001B[0;32m    841\u001B[0m p \u001B[38;5;241m=\u001B[39m _parse_sub(source, state, sub_verbose, nested \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    842\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m source\u001B[38;5;241m.\u001B[39mmatch(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 843\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m source\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmissing ), unterminated subpattern\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    844\u001B[0m                        source\u001B[38;5;241m.\u001B[39mtell() \u001B[38;5;241m-\u001B[39m start)\n\u001B[0;32m    845\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m group \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    846\u001B[0m     state\u001B[38;5;241m.\u001B[39mclosegroup(group, p)\n",
      "\u001B[1;31merror\u001B[0m: missing ), unterminated subpattern at position 11"
     ]
    }
   ],
   "source": [
    "\n",
    "count_true = 0\n",
    "for i in range(len(data)):\n",
    "    text = data[i]['text']\n",
    "    entit = data[i]['extracted_part'].get('text')\n",
    "    if entit:\n",
    "        entit = entit[0]\n",
    "    else:\n",
    "        entit =''\n",
    "\n",
    "    if pipe(text):\n",
    "        #print(pipe(text))\n",
    "        #pred = str(pipe(text)[0].get('word'))\n",
    "        #print('Предсказанная строка: ', pred)\n",
    "        pred = preprocessing_pred(pipe(text))\n",
    "        #print(pattern)\n",
    "        #print('После поиска: ', pred, end='\\n\\n')\n",
    "    else:\n",
    "        pred = ''\n",
    "\n",
    "\n",
    "    if entit == pred:\n",
    "        count_true+=1\n",
    "print(count_true/len(data))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обеспечения исполнения контракта 6593. 25 Российский рубль\n",
      "Размер.{1,}рубль\n",
      "Размер обеспечения исполнения контракта 6593.25 Российский рубль\n",
      "(1279, 1343)\n"
     ]
    }
   ],
   "source": [
    "s = 'Размер обеспечения исполнения контракта устанавливается в размере 5 ( пять ) процентов от цены, по которой заключается контракт и составляет _ _ _ _ _ _ _ _ _ ( _ _ _ _ _'\n",
    "\n",
    "s = 'Размер обеспечение исполнения контракта устанавливается от 0, 5 % до 30 % начальной ( максимальной ) цены контракта'\n",
    "\n",
    "s = 'Размер обеспечения исполнения контракта 6593. 25 Российский рубль'\n",
    "s = s.replace('( ', '(')\n",
    "s = s.replace('_ ', '_')\n",
    "s = s.replace(' )', ')')\n",
    "\n",
    "print(s)\n",
    "splt = s.split()\n",
    "pattern = splt[0]+'.{1,}'+splt[-1]\n",
    "print(pattern)\n",
    "pred = re.search(pattern, data[0]['text'])\n",
    "print(pred[0])\n",
    "print(pred.span())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (823) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[71], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m tokens \u001B[38;5;241m=\u001B[39m tokenizer(text, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m tokens \u001B[38;5;241m=\u001B[39m {k: v\u001B[38;5;241m.\u001B[39mto(model\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m tokens\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m----> 6\u001B[0m pred \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtokens)\n\u001B[0;32m      7\u001B[0m indices \u001B[38;5;241m=\u001B[39m pred\u001B[38;5;241m.\u001B[39mlogits\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m      9\u001B[0m token_text \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mconvert_ids_to_tokens(tokens[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32mD:\\Jupiter\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\Jupiter\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1760\u001B[0m, in \u001B[0;36mBertForTokenClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1754\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1755\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[0;32m   1756\u001B[0m \u001B[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001B[39;00m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1760\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1761\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1762\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1763\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1764\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1765\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1766\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1767\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1768\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1769\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1770\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1772\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1774\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(sequence_output)\n",
      "File \u001B[1;32mD:\\Jupiter\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\Jupiter\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1012\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1005\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[0;32m   1006\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[0;32m   1007\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[0;32m   1008\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[0;32m   1009\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[0;32m   1010\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m-> 1012\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1013\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1014\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1015\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1016\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1017\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1018\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1019\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[0;32m   1020\u001B[0m     embedding_output,\n\u001B[0;32m   1021\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1029\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m   1030\u001B[0m )\n\u001B[0;32m   1031\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mD:\\Jupiter\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\Jupiter\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:236\u001B[0m, in \u001B[0;36mBertEmbeddings.forward\u001B[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001B[0m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabsolute\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    235\u001B[0m     position_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embeddings(position_ids)\n\u001B[1;32m--> 236\u001B[0m     embeddings \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m position_embeddings\n\u001B[0;32m    237\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(embeddings)\n\u001B[0;32m    238\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(embeddings)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (823) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Загрузка модели"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ner_bert_20.bin\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ner_bert_20.bin\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 312,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"I-PWO\",\n",
      "    \"1\": \"O\",\n",
      "    \"2\": \"B-PWO\",\n",
      "    \"3\": \"I-EC\",\n",
      "    \"4\": \"B-EC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"label2id\": {\n",
      "    \"B-EC\": 4,\n",
      "    \"B-PWO\": 2,\n",
      "    \"I-EC\": 3,\n",
      "    \"I-PWO\": 0,\n",
      "    \"O\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29564\n",
      "}\n",
      "\n",
      "loading configuration file ner_bert_20.bin\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ner_bert_20.bin\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 312,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"I-PWO\",\n",
      "    \"1\": \"O\",\n",
      "    \"2\": \"B-PWO\",\n",
      "    \"3\": \"I-EC\",\n",
      "    \"4\": \"B-EC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"label2id\": {\n",
      "    \"B-EC\": 4,\n",
      "    \"B-PWO\": 2,\n",
      "    \"I-EC\": 3,\n",
      "    \"I-PWO\": 0,\n",
      "    \"O\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29564\n",
      "}\n",
      "\n",
      "loading weights file ner_bert_20.bin\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at ner_bert_20.bin.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "pip_2 = pipeline(task = 'ner', model='ner_bert_20.bin', tokenizer='ner_bert_tok.bin', aggregation_strategy='average', device=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обеспечения исполнения контракта 6593. 25 Российский рубль\n",
      "должен предоставить обеспечение исполнения контракта в размере 10 % от цены Контракта.\n",
      "исполнения контракта\n",
      "исполнения контракта\n",
      "00 %\n",
      "Размер обеспечения исполнения контракта устанавливается в размере 5 ( пять ) процентов от цены, по которой заключается контракт и составляет _ _ _ _ _ _ _ _ _\n",
      "Размер обеспечение исполнения контракта устанавливается от 0, 5 % до 30 % начальной ( максимальной ) цены контракта\n",
      "обеспечения\n",
      "Размер обеспечение исполнения контракта устанавливается от 0, 5 % до 30 % начальной ( максимальной ) цены контракта\n",
      "Размер обеспечение исполнения контракта устанавливается от 0, 5 % до 30 % начальной ( максимальной ) цены контракта\n"
     ]
    }
   ],
   "source": [
    "count_true = 0\n",
    "for i in range(len(data[:10])):\n",
    "    text = data[i]['text']\n",
    "    pred = str(pip_2(text)[0]['word'])\n",
    "    print(pred)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}